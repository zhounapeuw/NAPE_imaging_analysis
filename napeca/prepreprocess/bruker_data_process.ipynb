{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bruker meta data and raw ome-tiff preparation for preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds any .tif, .tiff, or ome.tiff files in the requested directory and first concatenates individual images into a single h5.\n",
    "\n",
    "If the user collected analog recordings from the Bruker microscope, the user can set a flag to process those analog signals and extract event times.\n",
    "\n",
    "If the user performed optogenetic stimulation (mark points), the user can set a flag to process the data and extract frame times where stimulation occurred as well as plot the mark points ROIs on the mean image.\n",
    "\n",
    "** NOTE: Raw ome-tiff images from Bruker need to be in the root directory (fdir) along with any xml meta data files.\n",
    "\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "__In this jupyter notebook, just run all cells in order (shift + enter). When you reach the last cell, it will prompt the user for input. You have two options:__\n",
    "\n",
    "1) __Edit the parameters in define_params to analyze a single session:__ First make edits to the parameters in the __second cell of this notebook (in the define_params function)__. In the last line of the same cell, change the argument string argument for the __method parameter to 'single'__. Then run all the cells of this jupyter notebook in order (shift + enter).\n",
    "\n",
    "2) You can also indicate specific files, parameters, and processing steps to include by __editing the python script called files_to_analyze_preprocess.py__ (in the same directory as this notebook). Follow the directions in that code; briefly you should first define the general parameters that will be applied to each session, then make additional dictionaries in the 'individual_files' list for each session to be analyzed. \n",
    "__A)__ Once you have specified the files and parameters in files_to_analyze_preprocess.py and saved the edits: In the last line of the 2nd cell of this notebook, change the argument string argument for the __method parameter to 'f2a'__\n",
    "__B)__ Then execute all the cells in this notebook in order; this code will automatically load the information in files_to_analyze_preprocess.py.\n",
    "\n",
    "\n",
    "Required Packages\n",
    "-----------------\n",
    "Python 2.7, scipy, h5py, multiprocessing, matplotlib, PIL, tifffile, lxml, pandas\n",
    "\n",
    "Custom code requirements: bruker_marked_pts_process, files_to_analyze_preprocess, utils_bruker\n",
    "\n",
    "Parameters (Only relevant if using the subfunction batch_process; ignore if using files_to_analyze or using default params by inputting a file directory)\n",
    "----------\n",
    "\n",
    "fname : string\n",
    "    name the session\n",
    "\n",
    "fdir : string\n",
    "    root file directory containing the raw ome-tiff files. Note: leave off the last backslash. For example: r'C:\\Users\\my_user\\analyze_sessions'\n",
    "\n",
    "Optional Parameters\n",
    "-------------------\n",
    "\n",
    "#### parameters for stitching bruker ome-tiffs to h5/tiffstack\n",
    "flag_make_h5_tiff : boolean  \n",
    "  * set as true if ome-tiffs need to be concatenated into an h5\n",
    "  \n",
    "save_type: string  \n",
    "  * Set as 'h5' or 'tif'\n",
    "\n",
    "number_frames: None or int  \n",
    "  * Optional; number of frames to analyze; defaults to analyzing whole session (None)\n",
    "\n",
    "flag_bruker_analog: boolean\n",
    "  * Set to true if analog/voltage input signals are present and are of interest\n",
    "  \n",
    "flag_bruker_stim : boolean\n",
    "  * Set to true if mark points SLM stim was performed\n",
    "  \n",
    "#### General analog processing variables\n",
    "analog_names : list of strings\n",
    "  * Strings should correspond to the TTL names for each analog channel; will replace default channel names based on numbering\n",
    "\n",
    "analog_ttl_thresh : int/float\n",
    "  * Threshold value for detecting TTL onsets in analog data; The diff/velocity is calculated, peaks that cross this threshold are counted as event onsets \n",
    "\n",
    "flag_validation_plots : boolean \n",
    "  * Set to true if want to plot traces of ttl pulses for visualizing and validating. Channel to plot is valid_plot channel\n",
    "  \n",
    "valid_plot_channel : string  \n",
    "  * Analog dataframe column names get cleaned up; AI's are \"input_#\"\n",
    "                \n",
    "flag_multicondition_analog : boolean\n",
    "  * Set to true if a single analog channel contains multiple conditions\n",
    "  \n",
    "  defaults to False\n",
    "  \n",
    "ai_to_split : int  \n",
    "  * analog port number that contains TTLs of multiple conditions; events here will be split into individual conditions if flag_multicondition_analog is set to true\n",
    "\n",
    "behav_id_of_interest: list of strings/ints\n",
    "  * Entries of this list must correspond to IDs used in the excel matching event names and id's (key_event.xlsx). eg. [101,102,103]\n",
    "  \n",
    "flag_plot_stim_threshold: Boolean\n",
    "  * Set to true to detect frames during which optogenetic stimulation occurred (useful for setting stimmed frames to NaN if there are stim artifacts\n",
    "\n",
    "flag_plot_stim_locs: Boolean\n",
    "  * Set to true to save a plot of the mean projection image with stimulation location contours overlaid\n",
    "  \n",
    "stim_frame_threshold: int/float\n",
    "  * Stimmed frames from opto are detected based on frame-averaged pixel values. Since stimmed frames have blanked lines, pixel-averaged fluorescence should be low. This is the threshold in std dev below mean for detecting stimmed frames via pixel-avged fluorescence (works if pmt is blanked during stim).\n",
    "    \n",
    "Output\n",
    "-------\n",
    "motion corrected file (in the format of h5) with \"\\_sima_mc\" appended to the end of the file name\n",
    "\n",
    "\"\\*\\_.h5\" : h5 file\n",
    "  * h5 file containing imaging frame data under the key 'imaging'\n",
    "\n",
    "\"\\framenumberforevents_*\\_.pkl\" : pickle file\n",
    "  * Only outputted if flag_bruker_analog is set to True\n",
    "  * pickle file containing dict with trial conditions as keys and lists of frame times for each event as values\n",
    "  \n",
    "\"\\*\\_stimmed_frames.pkl\" : pickle file\n",
    "  * Only outputted if flag_bruker_stim is set to True\n",
    "  * pickle file containing a dict. Keys are 'samples' and 'times' which contain numpy arrays listing the samples/times where stimulation occurred\n",
    "\n",
    "\"\\*.json\" : json file\n",
    "  * file containing the analysis parameters (fparams). Set by files_to_analyze_preprocess.py or default parameters.\n",
    "  * to view the data, one can easily open in a text editor (eg. word or wordpad).\n",
    "\n",
    "output_images : folder containing images  \n",
    "    You will also find a folder containing plots that reflect how each executed preprocessing step performed. Examples are mean images for motion corrected data, ROI masks overlaid on mean images, extracted signals for each ROI, etc..\n",
    "\n",
    "note: * is a wildcard indicating additional characters present in the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### packages for raw video to h5 processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# pip-installed packages\n",
    "from scipy import signal\n",
    "import h5py\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL.TiffTags import TAGS\n",
    "import tifffile as tiff\n",
    "from lxml.html.soupparser import fromstring\n",
    "from lxml.etree import tostring\n",
    "\n",
    "# custom code\n",
    "import bruker_marked_pts_process\n",
    "import files_to_analyze_preprocess\n",
    "\n",
    "#### more packages for xml meta and analog input processing\n",
    "import pickle\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# custom code\n",
    "import utils_bruker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "User-defined variables\n",
    "\"\"\"\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams = [\n",
    "            {\n",
    "                # ONLY EDIT LINES BELOW THIS COMMENT (parameters for analyzing a single session)\n",
    "                \n",
    "                # parameters for stitching bruker ome-tiffs to h5/tiffstack\n",
    "                'fname': '43384-1 400pwr 1104um 16x obj water-004',   # \n",
    "                'fdir':  r'D:\\bruker_data\\Sean\\43384-1 400pwr 1104um 16x obj water-004', #\n",
    "                'save_type': 'h5',\n",
    "                'fs': 7.5,\n",
    "                'number_frames': None, # optional; number of frames to analyze; defaults to analyzing whole session (None)\n",
    "                \n",
    "                # flags for performing the main sub-analyses\n",
    "                'flag_make_h5_tiff': False,\n",
    "                'flag_bruker_analog': True, # set to true if analog/voltage input signals are present and are of interest\n",
    "                'flag_bruker_stim': False, # set to true if mark points SLM stim was performed\n",
    "\n",
    "                # general analog processing variables\n",
    "                'analog_names': ['stim', 'frames', 'licks', 'rewards'],\n",
    "                'analog_ttl_thresh': 3, # threshold in voltage to detect TTL onset\n",
    "                # variables for plotting TTL pulses\n",
    "                'flag_validation_plots': True, # set to true if want to plot traces of ttl pulses for visualizing and validating\n",
    "                'valid_plot_name': 'rewards', # analog dataframe column names get cleaned up; AI's are \"input_#\"\n",
    "                \n",
    "                # variables for splitting analog channels encoding multiple conditions\n",
    "                'flag_multicondition_analog': False,\n",
    "                'ai_to_split': 2, # int, analog port number that contains TTLs of multiple conditions; events here will be split into individual conditions if flag_multicondition_analog is set to true\n",
    "                'behav_id_of_interest': [101,102,103],\n",
    "                # generate trial condition name list for whole session (for parameterization session)\n",
    "                'flag_parameterization_session': False,\n",
    "                'trial_condition_list': ['10_roi', '04_hz', '15_hz', '20_roi', '01_roi', \n",
    "                                         '90_pow', '03_roi', '03_pulse', '04_spirals', '10_pulse', \n",
    "                                         '01_pulse', '10_hz', '40_pow', '05_spirals', '05_pulse', \n",
    "                                         'default', '70_pow', '30_hz', '1_dot_5_hz', '30_roi', \n",
    "                                         '50_pow', '80_pow', '02_spirals'] * 3,\n",
    "\n",
    "                # variables for stimulation detection\n",
    "                'flag_plot_stim_threshold': True, # boolean to plot the 2p pixel-avg tseries with threshold for detecting stimmed blank frames\n",
    "                'flag_plot_stim_locs': True,\n",
    "                'stim_frame_threshold': 1 # threshold in std dev below mean for detecting stimmed frames via pixel-avged fluorescence (works if pmt is blanked during stim)\n",
    "                \n",
    "                # ONLY EDIT LINES ABOVE THIS COMMENT\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams_with_general = files_to_analyze_preprocess.define_fparams()\n",
    "        # lines below merge general params for each individual session to analyze\n",
    "        # merge_two_dicts: variables existing in 2nd arg are retained\n",
    "        fparams = [merge_two_dicts(fparams_with_general['general_params'], this_file_dict) \n",
    "                   for this_file_dict in fparams_with_general['individual_files']]\n",
    "    \n",
    "    elif method == 'csv':\n",
    "        \n",
    "        # easier to load the csv to dataframe then convert to dict\n",
    "        fparams=pd.read_csv('files_to_analyze_prepreprocess.csv').to_dict('record') \n",
    "        \n",
    "        # lists get loaded in as strings, so these are the keys that contain strings that need to be executed to form lists\n",
    "        eval_keys = ['analog_names', 'trial_condition_list', 'behav_id_of_interest', 'number_frames']\n",
    "        for idx, fparam in enumerate(fparams):\n",
    "            for eval_key in eval_keys:\n",
    "                if eval_key in fparam and isinstance(fparam[eval_key], str) :\n",
    "                    fparam[eval_key] = eval(fparam[eval_key])\n",
    "            fparams[idx] = fparam\n",
    "    \n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return fparams\n",
    "\n",
    "# USER CAN EDIT LINE BELOW THIS COMMENT TO CHANGE SESSION DEFINING METHOD\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir', 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions for raw video conversion to h5\n",
    "\n",
    "# to save json of manual params\n",
    "def save_json_dict(savedir, fname, dict_):\n",
    "\n",
    "    savepath = os.path.join(savedir, fname + '.json')\n",
    "\n",
    "    with open(savepath, 'w') as fp:\n",
    "        json.dump(dict_, fp)\n",
    "\n",
    "\n",
    "def read_shape_tiff(data_path):\n",
    "    \n",
    "    data = uint16_scale(tiff.imread(data_path)).astype('uint16')\n",
    "    data_shape = data.shape\n",
    "\n",
    "    return data, data_shape\n",
    "\n",
    "\n",
    "def get_ometif_xy_shape(fpath):\n",
    "    # read first tiff to get data shape\n",
    "    first_tif = tiff.imread(fpath, key=0, is_ome=True)\n",
    "    return first_tif.shape\n",
    "\n",
    "\n",
    "# ome-tiff contains meta data - read this in \n",
    "def get_tif_meta(tif_path):\n",
    "    meta_dict = {}\n",
    "    # iterate through metadata and create dict for key/value pairs\n",
    "    with Image.open(tif_path) as img:\n",
    "        for key in img.tag.iterkeys():\n",
    "            if key in TAGS:\n",
    "                meta_dict[TAGS[key]] = img.tag[key] \n",
    "            else:\n",
    "                meta_dict[key] = img.tag[key] \n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def assert_bruker(fpath):\n",
    "    meta_dict = get_tif_meta(fpath)\n",
    "    assert ('Prairie' in meta_dict['Software'][0]), \"This is not a bruker file!\"\n",
    "    \n",
    "\n",
    "# function to load individual ome-tiffs and add them to the tif-stack or h5 handle\n",
    "def load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_format):\n",
    "    # go through each chunk, load frames in chunk, process, and append to file\n",
    "    for idx, chunk_frames in enumerate(chunked_frame_idx):\n",
    "        print( 'Processing chunk {} out of {} chunks'.format(str(idx+1), str(len(chunked_frame_idx))) )\n",
    "        start_idx = chunk_frames[0]\n",
    "        end_idx = chunk_frames[-1]+1\n",
    "\n",
    "        data_to_save = tiff.imread(glob_list[start_idx:end_idx], key=0, is_ome=True) # key indicates the page in the tiff\n",
    "\n",
    "        if save_format == 'tif':\n",
    "\n",
    "            for frame in data_to_save:\n",
    "                save_object.save(frame, photometric='minisblack')\n",
    "\n",
    "        # https://stackoverflow.com/questions/25655588/incremental-writes-to-hdf5-with-h5py\n",
    "        elif save_format == 'h5':   \n",
    "\n",
    "            # append data to h5    \n",
    "            save_object[start_idx:end_idx] = data_to_save\n",
    "\n",
    "# main function to prepare list of ome-tiffs in directory and handles for stacking the tiffs            \n",
    "def main_ometif_to_composite(fdir, fname, save_type='h5', num_frames=None):\n",
    "\n",
    "    save_fname = os.path.join(fdir, fname)\n",
    "    glob_list = sorted(glob.glob(os.path.join(fdir,\"*.tif\")))\n",
    "    dtype_img = tiff.imread(glob_list[0], key=0, is_ome=True).dtype # get the data type from first frame\n",
    "\n",
    "    # get frame info\n",
    "    if not num_frames: \n",
    "        num_frames = len(glob_list)\n",
    "    frame_shape = get_ometif_xy_shape(glob_list[0])\n",
    "    print(str(num_frames) + ' total frame(s)')\n",
    "\n",
    "    # prepare to split data into chunks when loading to reduce memory imprint\n",
    "    chunk_size = 10000.0\n",
    "    n_chunks = int(np.ceil(num_frames/chunk_size))\n",
    "    chunked_frame_idx = np.array_split(np.arange(num_frames), n_chunks) # split frame indices into chunks\n",
    "\n",
    "    assert_bruker(glob_list[0])\n",
    "    print('Processing Bruker data')\n",
    "\n",
    "    # prepare handles to write data to\n",
    "    if save_type == 'tif':\n",
    "        save_object = tiff.TiffWriter(save_fname + '.tif', bigtiff=True)\n",
    "    elif save_type == 'h5':\n",
    "        f = h5py.File(save_fname + '.h5', 'w')\n",
    "        # get data shape and chunk up data, and initialize h5 \n",
    "        save_object = f.create_dataset('imaging', (num_frames, frame_shape[0], frame_shape[1]), \n",
    "                                maxshape=(None, frame_shape[0], frame_shape[1]), dtype=dtype_img)\n",
    "        \n",
    "    load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_type)\n",
    "    \n",
    "    if save_type == 'h5':\n",
    "        f.close()\n",
    "\n",
    "####### functions for meta data xml processing and analog processing ##########\n",
    "\n",
    "# make a dict of possible paths for loading and saving\n",
    "def bruker_analog_define_paths(fdir, fname):\n",
    "    paths_dict = {'fdir': fdir, 'fname': fname}\n",
    "    paths_dict['bruker_tseries_xml_path'] = os.path.join(fdir, fname + '.xml') # recording/tseries main xml\n",
    "    paths_dict['glob_analog_csv'] = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.csv\")) # grab all analog/voltage recording csvs\n",
    "    paths_dict['glob_analog_xml'] = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.xml\")) # grab all analog/voltage recording xml meta\n",
    "    # behavioral event identification files\n",
    "    paths_dict['behav_fname'] = fname + '_taste_reactivity.csv' # csv containing each behav event and corresponding sample\n",
    "    paths_dict['behav_event_key_path'] = r'D:\\bruker_data\\Adam\\key_event.xlsx' # location of excel matching event names and id's\n",
    "    # define save paths\n",
    "    paths_dict['fig_out_dir'] = os.path.join(fdir, '{}_output_images'.format(fname) )\n",
    "    paths_dict['behav_save_path'] = os.path.join(fdir, 'framenumberforevents_{}.pkl'.format(fname) )\n",
    "    paths_dict['behav_save_path_time'] = os.path.join(fdir, 'event_times_{}.pkl'.format(fname) )\n",
    "    paths_dict['behav_analog_save_path'] = os.path.join(fdir, 'framenumberforevents_analog_{}.pkl'.format(fname) )\n",
    "     \n",
    "    return paths_dict\n",
    "\n",
    "\n",
    "# load in recording/tseries main xml and grab frame period\n",
    "def bruker_xml_get_2p_fs(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    for child in list(xml_parse.findall('PVStateShard')[0]):\n",
    "        if 'framePeriod' in ET.tostring(child):\n",
    "            return 1.0/float(child.attrib['value'])\n",
    "\n",
    "        \n",
    "# takes bruker xml data, parses for each frame's timing and cycle\n",
    "def bruker_xml_make_frame_info_df(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    frame_info_df = pd.DataFrame()\n",
    "    for idx, type_tag in enumerate(xml_parse.findall('Sequence/Frame')):\n",
    "        # extract relative and absolute time from each frame's xml meta data\n",
    "        frame_info_df.loc[idx, 'rel_time'] = float(type_tag.attrib['relativeTime'])\n",
    "        frame_info_df.loc[idx, 'abs_time'] = float(type_tag.attrib['absoluteTime'])\n",
    "\n",
    "        # grab cycle number from frame's name\n",
    "        frame_fname = type_tag.findall('File')[0].attrib['filename']\n",
    "        frame_info_df.loc[idx, 'cycle_num'] = int(re.findall('Cycle(\\d+)', frame_fname)[0])\n",
    "    return frame_info_df\n",
    "\n",
    "\n",
    "# loads and parses the analog/voltage recording's xml and grabs sampling rate\n",
    "def bruker_analog_xml_get_fs(xml_fpath):\n",
    "    analog_xml = ET.parse(xml_fpath).getroot()\n",
    "    return float(analog_xml.findall('Experiment')[0].find('Rate').text)\n",
    "\n",
    "def read_analog_csv(csv_path):\n",
    "    cycle_df = pd.read_csv(csv_path)\n",
    "    cycle_df['Time(s)'] = cycle_df['Time(ms)'] / 1000.0\n",
    "    cycle_df['cycle_num'] = float(re.findall('Cycle(\\d+)', csv_path)[0]) # get cycle # from filename\n",
    "    return cycle_df\n",
    "\n",
    "\"\"\"\n",
    "Concatenate the analog input csv files if there are multiple cycles. Also make a dataframe with the \n",
    "raw analog tseries and thresholded tseries (for ttl onset detection)\n",
    "\"\"\" \n",
    "def bruker_concatenate_thresh_analog(fparams):\n",
    "    # grab all csv voltage recording csv files that aren't the concatenated full\n",
    "    glob_analog_csv = [f for f in glob.glob(os.path.join(fparams['fdir'],\"*_VoltageRecording_*.csv\")) if 'concat' not in f]\n",
    "    glob_analog_xml = glob.glob(os.path.join(fparams['fdir'],\"*_VoltageRecording_*.xml\"))\n",
    "\n",
    "    if not glob_analog_csv:\n",
    "        print('No analog csv detected or \\'concat\\' is in the name ')\n",
    "    \n",
    "    # xml's contain metadata about the analog csv; make sure sampling rate is consistent across cycles\n",
    "    analog_xml_fs = set(map(bruker_analog_xml_get_fs, glob_analog_xml)) # map grabs sampling rate across all cycle xmls; set finds all unique list entries  \n",
    "    if len(analog_xml_fs) > 1: \n",
    "          warnings.warn('Sampling rate is not consistent across cycles!')\n",
    "    else:\n",
    "        analog_fs = list(analog_xml_fs)[0]\n",
    "    \n",
    "    # cycle through analog csvs and append to a dataframe\n",
    "    analog_concat = pd.DataFrame()\n",
    "    for cycle_idx, cycle_path_csv in enumerate(glob_analog_csv):\n",
    "\n",
    "        cycle_df = self.read_analog_csv(cycle_path_csv)\n",
    "\n",
    "        if cycle_idx == 0: # initialize pd dataframe with first cycle's data\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values\n",
    "            analog_concat = cycle_df\n",
    "        else:\n",
    "            # since time resets for each cycle (if more than one), calculate cumulative time\n",
    "            last_cumulative_time = analog_concat['cumulative_time_ms'].iloc[-1]\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values + last_cumulative_time + 1 # add 1 so that new cycle's first sample isn't the same as the last cycle's last sample\n",
    "            analog_concat = analog_concat.append(cycle_df, ignore_index = True)\n",
    "    \n",
    "    # clean up column names\n",
    "    analog_concat.columns = analog_concat.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '_').str.replace(')', '')\n",
    "        \n",
    "    # loop through all analog columns and get the diff and threshold for event onsets\n",
    "    analog_column_names = [column for column in analog_concat.columns if 'input' in column] \n",
    "    for idx, analog_column_name in enumerate(analog_column_names):\n",
    "        ttl_thresh = fparams['analog_ttl_thresh']\n",
    "        if 'stim' in fparams['analog_names'] and idx == fparams['analog_names'].index('stim'): \n",
    "            ttl_thresh = 0.01\n",
    "            \n",
    "        print(analog_column_name)\n",
    "        print(ttl_thresh)\n",
    "        analog_concat[analog_column_name + '_diff'] = np.append(np.diff(analog_concat[analog_column_name]) > ttl_thresh, \n",
    "                                                                False) # add a false to match existing df length\n",
    "\n",
    "    # save concatenated analog csv        \n",
    "    save_full_csv_path = os.path.join(fparams['fdir'], fparams['fname'] + '_VoltageRecording_concat.csv')\n",
    "    analog_concat.to_csv(save_full_csv_path, index=False)\n",
    "\n",
    "    return analog_concat\n",
    "\n",
    "\n",
    "# function for finding the index of the closest entry in an array to a provided value\n",
    "def find_nearest_idx(array, value):\n",
    "\n",
    "    if isinstance(array, pd.Series):\n",
    "        idx = (np.abs(array - value)).idxmin()\n",
    "        return idx, array.index.get_loc(idx), array[idx] # series index, 0-relative index, entry value\n",
    "    else:\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Take in analog dataframe (contains analog tseries and thresholded boolean) and \n",
    "make dict of 2p frame times for each condition's event\n",
    "\"\"\" \n",
    "def match_analog_event_to_2p(imaging_info_df, analog_dataframe, rename_ports = None, flag_multicondition_analog = False): \n",
    "\n",
    "    event_img_sample = {} # will contain analog channel names as keys and 2p imaging frame numbers for each event/ttl onset\n",
    "    event_analog_samples = {}\n",
    "    all_diff_columns = [diff_column for diff_column in analog_dataframe.columns if 'diff' in diff_column] # grab all diff'd analog column names\n",
    "    \n",
    "    if rename_ports:\n",
    "        if len(rename_ports) != len(all_diff_columns):\n",
    "            warnings.warn('Number of ports to rename doesn\\'t match with actual number of ports! Only renaming available number of ports')\n",
    "    \n",
    "    for idx_ai, ai_diff in enumerate(sorted(all_diff_columns)):\n",
    "        \n",
    "        # if user gives ports to rename, grab port data name\n",
    "        if rename_ports:\n",
    "            ai_name = rename_ports[idx_ai]\n",
    "        else:\n",
    "            ai_name = ai_diff\n",
    "        \n",
    "        if flag_multicondition_analog: # if the trials in analog ports need to be split up later, make a subdict to accommodate conditions keys\n",
    "            event_img_sample[ai_name] = {}; analog_event_dict[ai_name]['all'] = []\n",
    "            event_analog_samples[ai_name] = {}; analog_event_samples[ai_name]['all'] = []\n",
    "        else:\n",
    "            event_img_sample[ai_name] = []\n",
    "            event_analog_samples[ai_name] = [] \n",
    "            \n",
    "        # grab analog samples where TTL onset occurred\n",
    "        # analog_df diff columns are booleans for each frame that indicate if TTL threshold crossed (ie. event occurred)\n",
    "        analog_events = analog_dataframe.loc[analog_dataframe[ai_diff] == True, ['time_s', 'cycle_num']] \n",
    "\n",
    "        # for each detected analog event, find nearest 2p frame index and add to analog event dict\n",
    "        for idx, analog_event in analog_events.iterrows():\n",
    "\n",
    "            # cycles indicate which iteration block/trial number; times reset for each cycle\n",
    "            this_cycle_imaging_info = imaging_info_df[imaging_info_df['cycle_num'] == analog_event['cycle_num']]\n",
    "            whole_session_idx, cycle_relative_idx, value = find_nearest_idx(this_cycle_imaging_info['rel_time'], analog_event['time_s'])\n",
    "\n",
    "            if flag_multicondition_analog:\n",
    "                event_img_sample[ai_name]['all'].append(whole_session_idx)\n",
    "                event_analog_samples[ai_name]['all'].append(idx) \n",
    "            else:\n",
    "                event_img_sample[ai_name].append(whole_session_idx)\n",
    "                event_analog_samples[ai_name].append(idx) # this is used for analog channel plot validation\n",
    "    \n",
    "    return event_img_sample, event_analog_samples\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "if all behav events of interest (different conditions) are recorded on a single AI channel\n",
    "and need to reference the behavioral events csv to split conditions up\n",
    "\"\"\" \n",
    "def split_analog_channel(ai_to_split, behav_id_of_interest, fdir, behav_fname, behav_event_key_path, analog_event_dict):\n",
    "\n",
    "    unicode_to_str = lambda x:str(x) # just a simple function to convert unicode to string; \n",
    "\n",
    "    this_ai_to_split = [analog_diff_name for analog_diff_name in analog_event_dict.keys() if str(ai_to_split) in analog_diff_name][0]\n",
    "    \n",
    "    # load id's and samples (camera samples?) of behavioral events (output by behavioral program)\n",
    "    behav_df = pd.read_csv(os.path.join(fdir, behav_fname), names=['id', 'sample'])\n",
    "    behav_event_keys = pd.read_excel(behav_event_key_path)\n",
    "\n",
    "    # using the behav event id, grab the event name from the keys dataframe; names are in unicode, so have to convert to string\n",
    "    behav_name_of_interest = map(unicode_to_str, \n",
    "                                 behav_event_keys[behav_event_keys['event_id'].isin(behav_id_of_interest)]['event_desc'].values)\n",
    "\n",
    "    # go into ordered behavioral event df, grab the trials with condition IDs of 'behav_id_of_interest' in order\n",
    "    trial_ids = list(behav_df[behav_df['id'].isin(behav_id_of_interest)]['id'].values) # grab 101, 102, 103 trials in order\n",
    "    \n",
    "    # loop through behav conditions, and separate event times for the conglomerate event times in analog_event_dict\n",
    "    for behav_event_id, behav_event_name in zip(behav_id_of_interest, behav_name_of_interest):\n",
    "        this_event_idxs = [idx for idx,val in enumerate(trial_ids) if val==behav_event_id]\n",
    "        analog_event_dict[this_ai_to_split][behav_event_name] = [analog_event_dict[this_ai_to_split]['all'][idx] for idx in this_event_idxs]\n",
    "        # analog_event_dict ultimately contains 2p frame indices for each event categorized by condition\n",
    "\n",
    "    # save preprocessed behavioral event data\n",
    "    with open(behav_analog_save_path, 'wb') as handle:\n",
    "        pickle.dump(analog_event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "# two funtions below aid in splitting the stim channel for the specific stim parameterization session made by CZ\n",
    "def find_param_stim_bout_onsets(event_dict, fs):\n",
    "\n",
    "    # finds stim frames that come after an inter-pulse interval of 5 s (ie. the start pulse of a trial)\n",
    "    event_frames = [ event_dict['stim'][idx+1] for (idx,val) in enumerate(np.diff(event_dict['stim'])) if val > fs*5  ]\n",
    "    event_frames = [event_dict['stim'][0]] + event_frames # add the first pulse of the session (first trial)\n",
    "    event_dict.pop('stim', 'none'); # get rid of 'stim' key in dict that contains all conditions; no longer useful\n",
    "\n",
    "    return event_frames, event_dict\n",
    "\n",
    "\n",
    "def split_event_dict_key(event_name_list, event_frames, event_dict, paths_dict):\n",
    "\n",
    "    # create empty lists for each new trial condition (key)\n",
    "    for key in set(event_name_list):\n",
    "        event_dict[key] = []\n",
    "                                                        \n",
    "    # loop through trial cond list (must match length of start_stim_frames), and sort trials in dict\n",
    "    for idx, cond in enumerate(event_name_list):\n",
    "        event_dict[cond].append(event_frames[idx])\n",
    "\n",
    "    with open(paths_dict['behav_save_path'], 'wb') as handle:\n",
    "       pickle.dump(event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "# take in data from an analog input and plot detected ttls\n",
    "def plot_analog_validation(channel_name, AI_onsets, analog_tseries, analog_fs, thresh, save_dir = None):\n",
    "    # following is just for visualizing ttls; here make tiles for indexing and extracting ttl data in trial manner\n",
    "    num_AI = len(AI_onsets)\n",
    "    \n",
    "    if num_AI > 1:\n",
    "    \n",
    "        rel_ind_vec = np.arange(-0.3*analog_fs, 1*analog_fs, 1)\n",
    "        trial_tvec = np.linspace(-0.3, 1, len(rel_ind_vec))\n",
    "        rel_ind_tile = np.tile(rel_ind_vec, (num_AI,1))\n",
    "        AI_onset_tile = np.tile(AI_onsets, (len(rel_ind_vec),1)).T\n",
    "\n",
    "        # extract analog values across flattened trial indices, get values of series, then reshape to 2d array\n",
    "        AI_value_tile = analog_tseries[np.ndarray.flatten(AI_onset_tile + rel_ind_tile)].values.reshape(AI_onset_tile.shape)\n",
    "        if AI_value_tile.shape[0] == num_AI:\n",
    "            AI_value_tile = AI_value_tile.T\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "        ax[0].set_title('{} Full TTL series'.format(channel_name))\n",
    "        ax[0].plot(analog_tseries)\n",
    "        ax[0].plot(np.ones(len(analog_tseries))*thresh)\n",
    "\n",
    "        ax[1].set_title('{} ttls detected'.format(num_AI))\n",
    "        ax[1].plot(trial_tvec, AI_value_tile );\n",
    "        ax[1].set_xlabel('Time (s)')\n",
    "        ax[1].set_ylabel('Volts');\n",
    "\n",
    "        if save_dir:\n",
    "            utils_bruker.check_exist_dir(save_dir)\n",
    "            fig.savefig(os.path.join(save_dir, '{}_ttl_validation.png'.format(channel_name)));\n",
    "\n",
    "        \n",
    "def make_imaging_info_df(bruker_tseries_xml_path):\n",
    "    xml_parse = ET.parse(bruker_tseries_xml_path).getroot()\n",
    "    frame_info_df = pd.DataFrame()\n",
    "    type_tags = xml_parse.findall('Sequence/Frame')\n",
    "\n",
    "    # lambda function to take in a list of xml frame meta data and pull out timing and cycle info \n",
    "    grab_2p_xml_frame_time = lambda type_tag: [float(type_tag.attrib['relativeTime']), \n",
    "                                               float(type_tag.attrib['absoluteTime']),\n",
    "                                               int(re.findall('Cycle(\\d+)', type_tag.findall('File')[0].attrib['filename'])[0]) # first grab this frame's file name, then use regex to grab cycle number in the fname\n",
    "                                              ] \n",
    "\n",
    "    # make a dataframe of relative time, absolute time, cycle number for each frame\n",
    "    imaging_info_df = pd.DataFrame(map(grab_2p_xml_frame_time, type_tags), columns=['rel_time', 'abs_time', 'cycle_num'])\n",
    "\n",
    "    return imaging_info_df\n",
    "\n",
    "\n",
    "# load 2p recording meta xml and extract the info into a dict\n",
    "def bruker_make_2p_meta_dict(fdir, fname, paths_dict):\n",
    "    meta_2p_dict = {}\n",
    "    meta_2p_dict['fs_2p'] = bruker_xml_get_2p_fs(paths_dict['bruker_tseries_xml_path'])\n",
    "    # extract frame timing and cycle info into a pandas dataframe \n",
    "    meta_2p_dict['imaging_info_df'] = make_imaging_info_df(paths_dict['bruker_tseries_xml_path'])\n",
    "    # Parse main 2p time-series xml\n",
    "    meta_2p_dict['tvec_2p'] = meta_2p_dict['imaging_info_df']['rel_time']\n",
    "    meta_2p_dict['num_frames_2p'] = len(meta_2p_dict['tvec_2p'])\n",
    "\n",
    "    return meta_2p_dict\n",
    "\n",
    "\n",
    "# see description below: performs main lifting of analog data processing\n",
    "def bruker_process_analog_ttl(fparams, paths_dict, meta_2p_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    If you have analog signals, that indicate behavioral event onset, sent from your behavioral DAQ to the bruker GPIO box, the following code:\n",
    "\n",
    "    1) parses the analog voltage recording xmls \n",
    "    2) extracts the signals from the csvs\n",
    "    3) extracts the TTL onset times\n",
    "    4) and finally lines up which frame the TTL occurred on.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get analog data sampling rate from xml\n",
    "    analog_fs = bruker_analog_xml_get_fs(paths_dict['glob_analog_xml'][0])\n",
    "\n",
    "    ### either load concatenated voltage recording (across cycles), perform the concatenation, or load a single CSV (for single cycle)\n",
    "    volt_rec_full_path = os.path.join(fparams['fdir'], fparams['fname'] + '_VoltageRecording_concat.csv')\n",
    "    if os.path.exists(volt_rec_full_path): # if a trial-stitched, thresholded voltage recording was previously saved\n",
    "        print('Analog processing WARNING: Loading previous concatenated analog tseries; if analog detection params changed, delete _concat file and rereun')\n",
    "        analog_df = pd.read_csv(volt_rec_full_path)\n",
    "    else:\n",
    "        print('Analog processing: Processing analog tseries')\n",
    "        analog_df = bruker_concatenate_thresh_analog(fparams) \n",
    "\n",
    "    ### match analog ttl event onsets to the corresponding 2p frame (for each event in each analog port)\n",
    "    print('Analog processing: match event sample to 2p frame')\n",
    "    event_img_samples, event_analog_samples = match_analog_event_to_2p(meta_2p_dict['imaging_info_df'], \n",
    "                                                                       analog_df, rename_ports = fparams['analog_names'])\n",
    "\n",
    "    ### if there are multiple conditions signaled on a single analog port, split them up, resave as pickle\n",
    "    if fparams['flag_multicondition_analog']:\n",
    "        print('Splitting events on single channel')\n",
    "        split_analog_channel(fparams['ai_to_split'], fparams['fdir'], \n",
    "                             paths_dict['behav_fname'], paths_dict['behav_event_key_path'], event_img_samples) \n",
    "    \n",
    "    if fparams['flag_parameterization_session']:\n",
    "        start_stim_frames, event_img_samples = find_param_stim_bout_onsets(event_img_samples, fparams['fs'])        \n",
    "        split_event_dict_key(fparams['trial_condition_list'], start_stim_frames, event_img_samples, paths_dict)\n",
    "    \n",
    "    if fparams['flag_validation_plots']:\n",
    "        valid_save_dir = os.path.join(fparams['fdir'], fparams['fname'] + '_output_images')\n",
    "        utils_bruker.check_exist_dir(valid_save_dir)\n",
    "        for idx, analog_name in enumerate(fparams['analog_names']):\n",
    "            raw_channel_idx = idx+1 # this references the raw channel names (e.g. \"input_0\"); future should make key/value pairs of renamed channels\n",
    "            plot_analog_validation(analog_name, event_analog_samples[analog_name], analog_df.iloc[:, raw_channel_idx], \n",
    "                                   analog_fs, fparams['analog_ttl_thresh'], valid_save_dir);\n",
    "\n",
    "    # save preprocessed behavioral event data\n",
    "    with open(paths_dict['behav_save_path'], 'wb') as handle: # for event data in imaging sampling format\n",
    "        pickle.dump(event_img_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(paths_dict['behav_save_path_time'], 'wb') as handle: # for events in time\n",
    "        for key in event_img_samples:\n",
    "            event_img_samples[key] = np.array(event_img_samples[key])/fparams['fs']\n",
    "        pickle.dump(event_img_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "    \n",
    "\n",
    "# define paths and bruker xml meta data; then passes that meta data and analog voltage_recordings to identify event/TTL frames/times\n",
    "def main_bruker_analog(fparam):\n",
    "    # define file paths and output file names\n",
    "    paths_dict = bruker_analog_define_paths(fparam['fdir'], fparam['fname'])\n",
    "\n",
    "    # get more timing meta data about 2p frames from xmls (this file is collected with any bruker 2p tseries)\n",
    "    meta_2p_dict = bruker_make_2p_meta_dict(fparam['fdir'], fparam['fname'], paths_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    If you have analog signals, that indicate behavioral event onset, sent from your behavioral DAQ to the bruker GPIO box, the following code:\n",
    "\n",
    "    1) parses the analog voltage recording xmls \n",
    "    2) extracts the signals from the csvs\n",
    "    3) extracts the TTL onset times\n",
    "    4) and finally lines up which frame the TTL occurred on.\n",
    "    \"\"\"\n",
    "    if fparam['flag_bruker_analog']: \n",
    "        bruker_process_analog_ttl(fparam, paths_dict, meta_2p_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just an intermediate function to distribute fparam and other general variables to main subprocess functions\n",
    "def single_file_process(fparam):\n",
    "    \n",
    "    print('Prepreprocessing {}'.format(fparam['fname']))\n",
    "    save_json_dict(fparam['fdir'], fparam['fname']+'_fparam_preprocess', fparam)\n",
    "    \n",
    "    if fparam['flag_make_h5_tiff']:\n",
    "        main_ometif_to_composite(fparam['fdir'], fparam['fname'], fparam['save_type'], num_frames=fparam['number_frames'])\n",
    "    \n",
    "    # Meta, Analog/TTL, & Behavioral Data Preprocessing\n",
    "    if fparam['flag_bruker_analog']:\n",
    "        print('Processing analog TTLs; outputs framenumberforevents pickle file')\n",
    "        main_bruker_analog(fparam)\n",
    "\n",
    "    if fparam['flag_bruker_stim']:\n",
    "        print('Detecting stimulation times/frames; outputs _stimmed_frames.pkl file')\n",
    "        bruker_marked_pts_process.main_detect_save_stim_frames(fparam['fdir'], fparam['fname'], \n",
    "                                                               detection_threshold=fparam['stim_frame_threshold'], flag_plot_mk_pts=fparam['flag_plot_stim_locs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_files = len(fparams)\n",
    "if num_files == 0:\n",
    "    raise Exception(\"No files to analyze!\")\n",
    "print(str(num_files) + ' files to analyze')\n",
    "\n",
    "# determine number of cores to use and initialize parallel pool\n",
    "num_processes = min(mp.cpu_count(), num_files)\n",
    "print('Total CPU cores for parallel processing: ' + str(num_processes))\n",
    "pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "# perform parallel processing; pass iterable list of file params to the analysis module selection code\n",
    "#pool.map(single_file_process, fparams)\n",
    "\n",
    "## for testing\n",
    "for fparam in fparams:\n",
    "    single_file_process(fparam) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### some additional code to load event dictionary to check detected events\n",
    "\n",
    "fdir = fparams[0]['fdir']\n",
    "fname = fparams[0]['fname']\n",
    "event_fname = 'event_times_{}.pkl'.format(fname)\n",
    "\n",
    "# reload event frame numbers\n",
    "with open(os.path.join(fdir, event_fname), 'rb') as handle:\n",
    "        events = pickle.load(handle)\n",
    "        \n",
    "len(events['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "846cf8043baeb41f766d16c6178fa5c082d7f5fd6394fad9f45cd658f981199e"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
